{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm, metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(500)\n",
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH =  \"train_disjoint.csv\" # \"datasets/articles/train_text_dataset.csv\" \n",
    "TEST_PATH = \"test_disjoint.csv\" # \"datasets/articles/test_text_dataset.csv\"  \n",
    "\n",
    "fields = [\"label\", \"article\"]\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH, usecols=fields)\n",
    "test_df = pd.read_csv(TEST_PATH, usecols=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleansing\n",
    "def cleansing(doc):\n",
    "    # Remove stop words\n",
    "    doc = [token for token in doc if not token.is_stop]\n",
    "    return doc\n",
    "\n",
    "def keep_specific_pos(doc, pos=[\"ADV\", \"ADJ\", \"VERB\", \"NOUN\"]):\n",
    "    doc = [token for token in doc if token.pos_ in pos]\n",
    "    return doc\n",
    "\n",
    "def preprocess(data):\n",
    "    docs = list(nlp.pipe(data))\n",
    "    preprocess_docs = [keep_specific_pos(cleansing(doc)) for doc in docs]\n",
    "    # Doc -> Text (+ lemmatization)\n",
    "    output_texts = [\" \".join([token.lemma_ for token in doc]) for doc in preprocess_docs]\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = train_df[\"label\"].values - 1, test_df[\"label\"].values - 1\n",
    "\n",
    "\n",
    "x_train = preprocess([str(text) for text in train_df[\"article\"].values])\n",
    "# np.save(\"datasets/articles/x_train.npy\", x_train)\n",
    "# x_train = np.load(\"datasets/articles/x_train.npy\")\n",
    "\n",
    "train = np.asarray((x_train, y_train)).T\n",
    "np.random.shuffle(train)\n",
    "x_train, y_train = np.array(train[:,0], dtype=str), np.array(train[:,1], dtype=int)\n",
    "\n",
    "STOP_LEMMA = [\"pourcent\", \"greenpeace\", \"réaliste\", \"fig\", \"vidéo\", \"climato\", \"régression\", \"climat\", \"réchauffement\", \"température\", \"scientifique\"]\n",
    "for i in range(len(x_train)):\n",
    "    for stop_lemma in STOP_LEMMA:\n",
    "        x_train[i] = x_train[i].replace(stop_lemma, '')\n",
    "\n",
    "\n",
    "\n",
    "x_test = preprocess([str(text) for text in test_df[\"article\"].values])\n",
    "# np.save(\"datasets/articles/x_test.npy\", x_test)\n",
    "# x_test = np.load(\"datasets/articles/x_test.npy\")\n",
    "for i in range(len(x_test)):\n",
    "    for stop_lemma in STOP_LEMMA:\n",
    "        x_test[i] = x_test[i].replace(stop_lemma, '')\n",
    "\n",
    "x_test, y_test = np.array(x_test, dtype=str), np.array(y_test, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_x_train = []\n",
    "e_x_train += list(x_train[y_train == 0][:1000])\n",
    "e_x_train += list(x_train[y_train == 1][:1000])\n",
    "e_x_train += list(x_train[y_train == 2][:1000])\n",
    "\n",
    "x_train = e_x_train\n",
    "\n",
    "e_y_train = []\n",
    "e_y_train += list(y_train[y_train == 0][:1000])\n",
    "e_y_train += list(y_train[y_train == 1][:1000])\n",
    "e_y_train += list(y_train[y_train == 2][:1000])\n",
    "\n",
    "y_train = e_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_x_test = []\n",
    "e_x_test += list(x_test[y_test == 0][:100])\n",
    "e_x_test += list(x_test[y_test == 1][:100])\n",
    "e_x_test += list(x_test[y_test == 2][:100])\n",
    "x_test = e_x_test\n",
    "\n",
    "e_y_test = []\n",
    "e_y_test += list(y_test[y_test == 0][:100])\n",
    "e_y_test += list(y_test[y_test == 1][:100])\n",
    "e_y_test += list(y_test[y_test == 2][:100])\n",
    "\n",
    "y_test = e_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train_bis, x_test_bis\n",
    "y_train, y_test = y_train_bis, y_test_bis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(x_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U50700'), dtype('<U50700')) -> dtype('<U50700')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-78191f4fb298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTfidf_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTfidf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mTrain_X_Tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTest_X_Tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U50700'), dtype('<U50700')) -> dtype('<U50700')"
     ]
    }
   ],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(x_train + x_test)\n",
    "Train_X_Tfidf = Tfidf_vect.transform(x_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {v: k for k, v in Tfidf_vect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,y_train)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)\n",
    "confusion_matrix = metrics.classification_report(y_test, predictions_SVM, zero_division=0)\n",
    "report = metrics.confusion_matrix(y_test, predictions_SVM)\n",
    "print(confusion_matrix)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(SVM.support_.shape[0]):\n",
    "    label = SVM.predict(SVM.support_vectors_.getrow(i))\n",
    "    word = id_to_word[SVM.support_vectors_.getrow(i).argmax()]\n",
    "    print(f'{label}: {word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_max = SVM.support_vectors_.sum(0).argsort().transpose()\n",
    "\n",
    "for i in range(1, len(arg_max) + 1):\n",
    "    id = int(arg_max[-i])\n",
    "    print(id_to_word[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cb844472113827ce6bd2403b167713163b10875d7f475488cf215bdab03c8f6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
